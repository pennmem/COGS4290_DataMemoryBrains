{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: This is only applicable to Rhino. The principles of parallel computing are general and can be used to accelerate code on any system, but the implementations here are designed to interact with the scheduler used on the Computational Memory Lab cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Parallel Computing with Dask\n",
    "\n",
    "<img src=\"https://hpc.llnl.gov/sites/default/files/styles/with_sidebar_1_up/public/nodesNetwork.gif?itok=TBqDQmx0\">\n",
    "\n",
    "A node is a like a computer within a much bigger computer!\n",
    "\n",
    "**CMLDask wrapper**:\n",
    "* See https://github.com/pennmem/cmldask for wrapper package created for convenience using Rhino + typical parallel setup for our analyses\n",
    "* Check out [Dask](https://docs.dask.org/en/stable/) and [Dask distributed](https://distributed.dask.org/en/stable/) for more documentation about the underlying implementation. In particular, we're using the [Futures API](https://docs.dask.org/en/stable/futures.html?highlight=futures) for managing parallel tasks and [dask-jobqueue](http://jobqueue.dask.org/en/latest/generated/dask_jobqueue.SGECluster.html#dask_jobqueue.SGECluster) to connect with the Sun Grid Engine (SGE) scheduler on rhino. \n",
    "* Note that this package is intended for convenience and will not accomodate very specific computational needs that demand complex parallel architectures. You will need to use dask directly and create your own `Client()` instances that are more closely tailored to your needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic usage works as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/global/Anaconda/2019-10/envs/cml37/lib/python3.7/site-packages/dask_jobqueue/core.py:20: FutureWarning: tmpfile is deprecated and will be removed in a future release. Please use dask.utils.tmpfile instead.\n",
      "  from distributed.utils import tmpfile\n"
     ]
    }
   ],
   "source": [
    "import cmldask.CMLDask as da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique port for rdehaan is 51474\n",
      "{'dashboard_address': ':51474'}\n",
      "To view the dashboard, run: \n",
      "`ssh -fN rdehaan@rhino2.psych.upenn.edu -L 7000:192.168.86.143:33610` in your local computer's terminal (NOT rhino) \n",
      "and then navigate to http://localhost:7000 in your browser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/global/Anaconda/2019-10/envs/cml37/lib/python3.7/site-packages/distributed/node.py:161: UserWarning: Port 51474 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 33610 instead\n",
      "  f\"Port {expected} is already in use.\\n\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cmldask\n",
    "from dask.distributed import wait, as_completed, progress\n",
    "\n",
    "def squared(x):\n",
    "    return x**2\n",
    "\n",
    "logdir = os.path.join(os.path.abspath(os.curdir), 'dask_appendix_logs')\n",
    "if not os.path.exists(logdir): os.mkdir(logdir)\n",
    "client = da.new_dask_client(\"test_dask\", \"1GB\", log_directory=logdir, max_n_jobs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home1/rdehaan/projects/DataMemoryBrains/dask_appendix_logs'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# where cluster run logs will be stored\n",
    "logdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that you need to explicitly shut down this client (`client.shutdown()`) or shut down your jupyter kernel before running this cell again, or you might leave workers \"stranded\" without access to them (since that access is provided by the Client you would overwrite)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have options when setting up your Dask cluster/client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_dask_client\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmemory_per_job\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_n_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mthreads_per_job\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mprocesses_per_job\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0madapt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mqueue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'RAM.q'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mwalltime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'1500000'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlocal_directory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlog_directory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mscheduler_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'dashboard_address'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m':51391'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Returns a new dask client instance with associated dashboard for\n",
       "monitoring jobs. The default method assumes a very basic use case - that\n",
       "is, embarassingly parallel tasks - in which the user simply wants to speed\n",
       "up independent computations by running them in parallel. There is just one\n",
       "single-threaded process per job. The thread/process parameters are\n",
       "configurable, but will lead to more complicated parallel executions\n",
       "which might be hard to track. Workers might share memory instead of\n",
       "being independent.\n",
       "Leave the defaults unless you have some specific reason to believe another\n",
       "configuration will help with your use case.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "job_name : gives every job this client runs a shared name. You can\n",
       "    subsequentlyadd prefixes to this name for individual jobs.\n",
       "memory_per_job : a string specifying how much memory each job needs.\n",
       "    Ex: \"1GB\" for 50 jobs would request 1GB of memory for each job,\n",
       "    for a total of 50 GB. If any one job exceeds 1GB, you will get a\n",
       "    memory error.\n",
       "max_n_jobs : maximum number of parallel processes (assuming\n",
       "    processes_per_job is 1). Common courtesy dictates that you avoid\n",
       "    running more than 100 jobs and hogging cluster resources (default: 100)\n",
       "threads_per_job : number of threads used per job (default: 1)\n",
       "adapt : boolean determining whether to use adaptive or manual scaling.\n",
       "queue : Sun Grid Engine queue to use (default: \"RAM.q\")\n",
       "walltime : timeout for cluster job, after which job is killed. Seconds,\n",
       "    or HH:MM:SS (default : 17 days)\n",
       "local_directory : directory for dask worker space, used internally\n",
       "log_directory : a directory to dump worker log outputs. Dumped to home\n",
       "    directory by default.\n",
       "scheduler_options : dict of arguments to pass to Dask scheduler directly.\n",
       "    See docs for more\n",
       "    info: https://docs.dask.org/en/latest/how-to/deploy-dask/python-advanced.html#distributed.Scheduler\n",
       "**kwargs\n",
       "\n",
       "Returns\n",
       "-------\n",
       "client : instance of dask.Client\n",
       "\u001b[0;31mFile:\u001b[0m      /usr/global/Anaconda/2019-10/envs/cml37/lib/python3.7/site-packages/cmldask/CMLDask.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "da.new_dask_client?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel computing tips\n",
    "\n",
    "* Jobs are still subject to memory limitations, so you may need to **break up large processes into smaller chunks.** For example, each job could correspond to analyzing one session, instead of one subject (this should be done, e.g., for all Morlet wavelet analyses). \n",
    "* It is often useful to save the output of each job in a dedicated directory, and sometimes useful to save intermediate values to aid in debugging or later nonparallel analyses. The Python \"os\" library can be helpful here. \n",
    "* **Be respectful!** There are only so many cores available to your classmates, the Kahana lab, and our collaborators across the country.\n",
    "* **Limit typical jobs to 5 cores or less**. Heavy usage means fewer resources for other users, and due to shared disk resources might actually slow down all jobs overall. Please ask for permission before using more.\n",
    "* You can always use the '**qdel**' command in Terminal, followed by your job number, to kill any of your old jobs that may be wasting rhino's resources. \n",
    "* Use the '**qstat**' command in Terminal to see cluster usage information for all users. The **'qstat | grep your_username'** command will just show the jobs associated with your username\n",
    "* Each rhino2 node has ~128 GB of memory and ~40 cores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Dask Futures Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c818d995d2245b0bc24aca8d066b1eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from time import sleep\n",
    "\n",
    "# functions to run on the cluster\n",
    "def add(a, b):\n",
    "    sleep(np.random.randint(0, 10))\n",
    "    return a + b\n",
    "\n",
    "def error_add(a, b):\n",
    "    sleep(np.random.randint(0, 10))\n",
    "    if a % 2:\n",
    "        raise ValueError\n",
    "    return a + b\n",
    "\n",
    "# launch the jobs\n",
    "futures = client.map(add, range(40), range(40))\n",
    "progress(futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.86.139:40800'\n",
      "distributed.diskutils - INFO - Found stale lock file and directory '/home1/rdehaan/dask-worker-space/dask-worker-space/worker-jiq0qy7u', purging\n",
      "distributed.worker - INFO -       Start worker at: tcp://192.168.86.139:35492\n",
      "distributed.worker - INFO -          Listening to: tcp://192.168.86.139:35492\n",
      "distributed.worker - INFO -          dashboard at:       192.168.86.139:38817\n",
      "distributed.worker - INFO - Waiting to connect to: tcp://192.168.86.143:37742\n",
      "distributed.worker - INFO - -------------------------------------------------\n",
      "distributed.worker - INFO -               Threads:                          1\n",
      "distributed.worker - INFO -                Memory:                   0.93 GiB\n",
      "distributed.worker - INFO -       Local Directory: /home1/rdehaan/dask-worker-space/dask-worker-space/worker-vfsoljtz\n",
      "distributed.worker - INFO - -------------------------------------------------\n",
      "distributed.worker - INFO -         Registered to: tcp://192.168.86.143:37742\n",
      "distributed.worker - INFO - -------------------------------------------------\n",
      "distributed.core - INFO - Starting established connection\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# just for illustration; the log files are just text files, so it's typically easier to open them up manually\n",
    "with open(os.path.join(logdir, os.listdir(logdir)[0]), 'r') as f:\n",
    "    log = f.read()\n",
    "print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0miterables\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mresources\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpriority\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mallow_other_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfifo_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'100 ms'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mactor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mactors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Map a function on a sequence of arguments\n",
       "\n",
       "Arguments can be normal objects or Futures\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "func : callable\n",
       "    Callable to be scheduled for execution. If ``func`` returns a coroutine, it\n",
       "    will be run on the main event loop of a worker. Otherwise ``func`` will be\n",
       "    run in a worker's task executor pool (see ``Worker.executors`` for more\n",
       "    information.)\n",
       "iterables : Iterables\n",
       "    List-like objects to map over.  They should have the same length.\n",
       "key : str, list\n",
       "    Prefix for task names if string.  Explicit names if list.\n",
       "pure : bool (defaults to True)\n",
       "    Whether or not the function is pure.  Set ``pure=False`` for\n",
       "    impure functions like ``np.random.random``.\n",
       "    See :ref:`pure functions` for more details.\n",
       "workers : string or iterable of strings\n",
       "    A set of worker hostnames on which computations may be performed.\n",
       "    Leave empty to default to all workers (common case)\n",
       "allow_other_workers : bool (defaults to False)\n",
       "    Used with `workers`. Indicates whether or not the computations\n",
       "    may be performed on workers that are not in the `workers` set(s).\n",
       "retries : int (default to 0)\n",
       "    Number of allowed automatic retries if a task fails\n",
       "priority : Number\n",
       "    Optional prioritization of task.  Zero is default.\n",
       "    Higher priorities take precedence\n",
       "fifo_timeout : str timedelta (default '100ms')\n",
       "    Allowed amount of time between calls to consider the same priority\n",
       "resources : dict (defaults to {})\n",
       "    Defines the `resources` each instance of this mapped task requires\n",
       "    on the worker; e.g. ``{'GPU': 2}``.\n",
       "    See :doc:`worker resources <resources>` for details on defining\n",
       "    resources.\n",
       "actor : bool (default False)\n",
       "    Whether these tasks should exist on the worker as stateful actors.\n",
       "    See :doc:`actors` for additional details.\n",
       "actors : bool (default False)\n",
       "    Alias for `actor`\n",
       "batch_size : int, optional\n",
       "    Submit tasks to the scheduler in batches of (at most) ``batch_size``.\n",
       "    Larger batch sizes can be useful for very large ``iterables``,\n",
       "    as the cluster can start processing tasks while later ones are\n",
       "    submitted asynchronously.\n",
       "**kwargs : dict\n",
       "    Extra keywords to send to the function.\n",
       "    Large values will be included explicitly in the task graph.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> L = client.map(func, sequence)  # doctest: +SKIP\n",
       "\n",
       "Returns\n",
       "-------\n",
       "List, iterator, or Queue of futures, depending on the type of the\n",
       "inputs.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "Client.submit : Submit a single function\n",
       "\u001b[0;31mFile:\u001b[0m      /usr/global/Anaconda/2019-10/envs/cml37/lib/python3.7/site-packages/distributed/client.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "client.map?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "futures = client.map(error_add, range(40), range(40))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait for all jobs to finish, check for errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exception</th>\n",
       "      <th>traceback_obj</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ValueError()</td>\n",
       "      <td>&lt;traceback object at 0x2aca5a032280&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ValueError()</td>\n",
       "      <td>&lt;traceback object at 0x2aca5a0bc140&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ValueError()</td>\n",
       "      <td>&lt;traceback object at 0x2aca5a13bcd0&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ValueError()</td>\n",
       "      <td>&lt;traceback object at 0x2aca5a10b640&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ValueError()</td>\n",
       "      <td>&lt;traceback object at 0x2aca5a146780&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ValueError()</td>\n",
       "      <td>&lt;traceback object at 0x2aca5a152e60&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ValueError()</td>\n",
       "      <td>&lt;traceback object at 0x2aca5a1889b0&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ValueError()</td>\n",
       "      <td>&lt;traceback object at 0x2aca5a134320&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ValueError()</td>\n",
       "      <td>&lt;traceback object at 0x2aca5a18cd70&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ValueError()</td>\n",
       "      <td>&lt;traceback object at 0x2aca601b1960&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ValueError()</td>\n",
       "      <td>&lt;traceback object at 0x2aca2b1ddaa0&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ValueError()</td>\n",
       "      <td>&lt;traceback object at 0x2aca5a161230&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ValueError()</td>\n",
       "      <td>&lt;traceback object at 0x2aca601a8370&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>ValueError()</td>\n",
       "      <td>&lt;traceback object at 0x2aca601e81e0&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>ValueError()</td>\n",
       "      <td>&lt;traceback object at 0x2aca5a1500a0&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>ValueError()</td>\n",
       "      <td>&lt;traceback object at 0x2aca601cc910&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>ValueError()</td>\n",
       "      <td>&lt;traceback object at 0x2aca6024adc0&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>ValueError()</td>\n",
       "      <td>&lt;traceback object at 0x2aca6020e460&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>ValueError()</td>\n",
       "      <td>&lt;traceback object at 0x2aca60245370&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>ValueError()</td>\n",
       "      <td>&lt;traceback object at 0x2aca60246460&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          exception                         traceback_obj\n",
       "param                                                    \n",
       "1      ValueError()  <traceback object at 0x2aca5a032280>\n",
       "3      ValueError()  <traceback object at 0x2aca5a0bc140>\n",
       "5      ValueError()  <traceback object at 0x2aca5a13bcd0>\n",
       "7      ValueError()  <traceback object at 0x2aca5a10b640>\n",
       "9      ValueError()  <traceback object at 0x2aca5a146780>\n",
       "11     ValueError()  <traceback object at 0x2aca5a152e60>\n",
       "13     ValueError()  <traceback object at 0x2aca5a1889b0>\n",
       "15     ValueError()  <traceback object at 0x2aca5a134320>\n",
       "17     ValueError()  <traceback object at 0x2aca5a18cd70>\n",
       "19     ValueError()  <traceback object at 0x2aca601b1960>\n",
       "21     ValueError()  <traceback object at 0x2aca2b1ddaa0>\n",
       "23     ValueError()  <traceback object at 0x2aca5a161230>\n",
       "25     ValueError()  <traceback object at 0x2aca601a8370>\n",
       "27     ValueError()  <traceback object at 0x2aca601e81e0>\n",
       "29     ValueError()  <traceback object at 0x2aca5a1500a0>\n",
       "31     ValueError()  <traceback object at 0x2aca601cc910>\n",
       "33     ValueError()  <traceback object at 0x2aca6024adc0>\n",
       "35     ValueError()  <traceback object at 0x2aca6020e460>\n",
       "37     ValueError()  <traceback object at 0x2aca60245370>\n",
       "39     ValueError()  <traceback object at 0x2aca60246460>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wait(futures)\n",
    "errors = da.get_exceptions(futures, range(40))\n",
    "errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick out the index where you want to view the traceback message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"<ipython-input-11-492535f77c2d>\", line 11, in error_add\n",
      "    raise ValueError\n"
     ]
    }
   ],
   "source": [
    "da.print_traceback(errors, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice gathering these doesn't work because there are errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-04978e95e583>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/global/Anaconda/2019-10/envs/cml37/lib/python3.7/site-packages/distributed/client.py\u001b[0m in \u001b[0;36mgather\u001b[0;34m(self, futures, errors, direct, asynchronous)\u001b[0m\n\u001b[1;32m   1973\u001b[0m                 \u001b[0mdirect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdirect\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1974\u001b[0m                 \u001b[0mlocal_worker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_worker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1975\u001b[0;31m                 \u001b[0masynchronous\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1976\u001b[0m             )\n\u001b[1;32m   1977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/global/Anaconda/2019-10/envs/cml37/lib/python3.7/site-packages/distributed/client.py\u001b[0m in \u001b[0;36msync\u001b[0;34m(self, func, asynchronous, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    864\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m             return sync(\n\u001b[0;32m--> 866\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    867\u001b[0m             )\n\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/global/Anaconda/2019-10/envs/cml37/lib/python3.7/site-packages/distributed/utils.py\u001b[0m in \u001b[0;36msync\u001b[0;34m(loop, func, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/global/Anaconda/2019-10/envs/cml37/lib/python3.7/site-packages/distributed/utils.py\u001b[0m in \u001b[0;36mf\u001b[0;34m()\u001b[0m\n\u001b[1;32m    308\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcallback_timeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m                 \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m             \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32myield\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m             \u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/global/Anaconda/2019-10/envs/cml37/lib/python3.7/site-packages/tornado/gen.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m                         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m                         \u001b[0mexc_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/global/Anaconda/2019-10/envs/cml37/lib/python3.7/site-packages/distributed/client.py\u001b[0m in \u001b[0;36m_gather\u001b[0;34m(self, futures, errors, direct, local_worker)\u001b[0m\n\u001b[1;32m   1832\u001b[0m                             \u001b[0mexc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1833\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1834\u001b[0;31m                             \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1835\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1836\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"skip\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-492535f77c2d>\u001b[0m in \u001b[0;36merror_add\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "client.gather(futures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, let's filter for successful ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56, 60, 64, 68, 72, 76]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_futures = da.filter_futures(futures)\n",
    "client.gather(good_futures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTANT: Shutdown your client (or restart your kernel, which will do so automatically)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Recommended Best Practices\n",
    "\n",
    "1. Watch your memory usage. Jobs that go over memory limits will error out. Delete unnecessary variables or overwrite variables to reduce memory overhead.\n",
    "2. Saving out **dimensionally reduced** intermediate outputs. \n",
    "    * Clusters can be unreliable and have outages, and jobs can get killed. Saving results from each job ensures that all results will not lost if a run dies partway through. \n",
    "    * You can write code to check if the results for a particular job have successfully returned and only recompute results for failed jobs.\n",
    "3. Avoid heavy disk usage by reading from and writing to a smaller number of medium to large-sized files rather than a large number of small files. The network/disk bandwidth on Rhino is a major bottleneck. If you use file IO (Input/Output, i.e., reading and writing to files) heavily, it will not only makes others' work slow, it can slow down your own analyses substantially.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example code with the Pickle library, which allows for saving and loading (most) objects in Python\n",
    "# the pickle library is not recommended for long-term storage since it depends on having the same object \n",
    "# code/package versions available when loading an object that were present when the object was saved\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "arr = np.arange(1000)\n",
    "\n",
    "# save output\n",
    "with open('arr_test.pkl', 'wb') as f:\n",
    "    pickle.dump(arr, f)\n",
    "    \n",
    "# load output\n",
    "with open('arr_test.pkl', 'rb') as f:\n",
    "    arr_loaded = pickle.load(f)\n",
    "\n",
    "# confirm that loaded == saved\n",
    "np.all(arr == arr_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Settings():\n",
    "  '''settings = Settings()\n",
    "     settings.somelist = [1, 2, 3]\n",
    "     settings.importantstring = 'saveme'\n",
    "     settings.Save()\n",
    "\n",
    "     settings = Settings.Load()\n",
    "  '''\n",
    "  def __init__(self, **kwargs):\n",
    "    for k,v in kwargs.items():\n",
    "      self.__dict__[k] = v\n",
    "\n",
    "  def Save(self, filename='settings.pkl'):\n",
    "    import pickle\n",
    "    with open(filename, 'wb') as fw:\n",
    "      fw.write(pickle.dumps(self))\n",
    "\n",
    "  def Load(filename='settings.pkl'):\n",
    "    import pickle\n",
    "    return pickle.load(open(filename, 'rb'))\n",
    "\n",
    "  def __repr__(self):\n",
    "    return ('Settings(' +\n",
    "      ', '.join(str(k)+'='+repr(v) for k,v in self.__dict__.items()) +\n",
    "      ')')\n",
    "\n",
    "  def __str__(self):\n",
    "    return '\\n'.join(str(k)+': '+str(v) for k,v in self.__dict__.items())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment: FR1\n",
      "logbase: totalwords_\n",
      "sub_list: ['R1341T', 'R1391T', 'R1395M', 'R1467M']\n"
     ]
    }
   ],
   "source": [
    "settings = Settings()\n",
    "settings.experiment = 'FR1'\n",
    "settings.logbase = 'totalwords_'\n",
    "settings.sub_list = ['R1341T', 'R1391T', 'R1395M', 'R1467M']\n",
    "\n",
    "settings.Save('totalwords.pkl')\n",
    "print(settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/global/Anaconda/2019-10/envs/cml37/lib/python3.7/site-packages/dask_jobqueue/core.py:20: FutureWarning: tmpfile is deprecated and will be removed in a future release. Please use dask.utils.tmpfile instead.\n",
      "  from distributed.utils import tmpfile\n"
     ]
    }
   ],
   "source": [
    "import cmldask.CMLDask as da\n",
    "from dask.distributed import wait, as_completed, progress\n",
    "from cmlreaders import CMLReader, get_data_index\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import traceback\n",
    "\n",
    "def TotalWords(sub):\n",
    "  logfile = f'{settings.logbase}{sub}.txt'\n",
    "  \n",
    "  df = get_data_index('all')\n",
    "  df = df[df['subject']==sub]\n",
    "  df = df[df['experiment']==settings.experiment]\n",
    "\n",
    "  total_words = 0\n",
    "  for df_sess in df.itertuples():\n",
    "    try:\n",
    "      df_sess = df_sess._asdict()\n",
    "      # Get parameters from the session DataFrame\n",
    "      exp = df_sess['experiment']\n",
    "      sess = df_sess['session']\n",
    "      # Prepare the reader\n",
    "      reader = CMLReader(sub, exp, sess, df_sess['montage'], df_sess['localization'])\n",
    "      # Get word events\n",
    "      evs = reader.load('task_events')\n",
    "      word_evs = evs[evs['type']=='WORD']\n",
    "      # Do our analysis!\n",
    "      total_words += len(word_evs)\n",
    "    except Exception as e:\n",
    "      # Log the exception to a subject-labeled filename,\n",
    "      # along with a label of subject, experiment, and session.\n",
    "      with open(logfile, 'a') as fw:\n",
    "        date = datetime.datetime.now().strftime('%F_%H-%M-%S')\n",
    "        fw.write(f'{date}: {sub}, {exp}, {sess}\\n' +\n",
    "                 ''.join(traceback.format_exception(type(e), e, e.__traceback__)))\n",
    "\n",
    "    \n",
    "  # Save the result.\n",
    "  np.save('wordcount_'+sub+'.npy', [total_words])\n",
    "  return total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique port for ryan.colyer is 51391\n",
      "{'dashboard_address': ':51391'}\n",
      "To view the dashboard, run: \n",
      "`ssh -fN ryan.colyer@rhino2.psych.upenn.edu -L 7000:192.168.86.143:51391` in your local computer's terminal (NOT rhino) \n",
      "and then navigate to http://localhost:7000 in your browser\n"
     ]
    }
   ],
   "source": [
    "# Run in parallel\n",
    "client = da.new_dask_client(\"test_dask\", \"4GB\", max_n_jobs=5)\n",
    "futures = client.map(TotalWords, settings.sub_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DoneAndNotDoneFutures(done={<Future: finished, type: int, key: TotalWords-926e2b75e568cfaf62c2df3f1b60f651>, <Future: finished, type: int, key: TotalWords-7bc1ebcfc280b90f3d50c762bf05a7f9>, <Future: finished, type: int, key: TotalWords-67821e81b45a688188187f1157db14d8>, <Future: finished, type: int, key: TotalWords-40a3f70f7b8f36a6ab1b4e3b44ad16ec>}, not_done=set())"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wait for results, check errors.\n",
    "wait(futures)\n",
    "#errors = da.get_exceptions(futures, range(40))\n",
    "#errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1260, 468, 936, 156]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_futures = da.filter_futures(futures)\n",
    "client.gather(good_futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R1341T had [1260] words\n",
      "R1391T had [468] words\n",
      "R1395M had [936] words\n",
      "R1467M had [156] words\n"
     ]
    }
   ],
   "source": [
    "settings = Settings.Load('totalwords.pkl')\n",
    "for sub in settings.sub_list:\n",
    "  total_words = np.load('wordcount_'+sub+'.npy')\n",
    "  print(f'{sub} had {total_words} words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Happy parallel computing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise: Write a parallel function that returns the number of (bipolar) electrodes for every subject in the RAM example dataset (ignore localization and montage details). Run with 5 jobs and 1 core per job. This will require you to integrate the data loading procedures you learned earlier with the dask framework exhibited using the toy examples above**\n",
    "\n",
    "This example is purely for illustration and this task is likely not much faster with multiple nodes than with a single node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshop",
   "language": "python",
   "name": "workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
